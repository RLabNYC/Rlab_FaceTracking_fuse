# Rlab_FaceTracking

Todd Bryant, Katt Sullivan and Grant Ng

# Facial and Motion Tracking
Now with ARKit and an iPhone with a front-facing True Depth camera you can track your facial features allowing you animate digital avatars. The avatars first have to be prepared with point-level animation called blendshapes.  ARKit uses 52 of these to animate the avatar faces.  3D modelling is a craft that takes a long time to master and setting up all of these blendshapes can be daunting, so we’ve developed a workflow to ease the barrier to get your character up and running in a matter of minutes by levering two free avatar creation tools: Adobe Fuse and MakeHuman. Both programs have more than enough blendshapes to animate a face, the just need to be renamed to the ARKit conventions. We’ve developed a simple script to automate of lot of this process.

Software needed: 
Fuse downloaded from here: https://www.adobe.com/products/fuse.html
Need to create a mixamo account (Its free!): https://www.mixamo.com/#/
MakeHuman downloaded here: http://www.makehumancommunity.org/
Maya student version download here: https://www.autodesk.com/education/free-software/maya
Unreal downloaded here: https://www.unrealengine.com/en-US/get-now/agnostic
Python download here: https://www.python.org/downloads/ **scroll all the way to see how to download properly!
For motion capture: 
Motive software and you need to have a space to do mocap. 
Software versions:
Unreal version 4.23
iPhone/iOS device at OS version 13.3 
Maya 2018 or 2019
Blender 2.82
Python 3.8  

This tutorial requires some knowledge of using 3D software and game engine mechanics. We will provide more links for beginners that walk you through a more in depth walkthrough. 

